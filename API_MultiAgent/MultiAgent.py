# I will be doing a multi-agent RAG system , consisting of 3 total agents
# Each agent will have its own RAG chain and will be specialized in a specific area
# Agent 1 will search for results on the rag used to search on the local documents
# Agent 2 will search for results on the web
# Agent 3 will be the final agent that will combine the results of the previous agents and generate the final answer
# We will also have an orchestrator agent that will decide which agent to use based on the question
# I will be using the langgraph library to create the multi-agent system
# I will also be using Milvus as the vector store for the local documents
# I am going to add FastAPI to this RAG. The idea is to receive a question,along with the system prompt 
# and return the answer generated by the multi-agent system via an API call
from pathlib import Path
from typing import List, Optional, TypedDict
import uuid
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from pydantic import BaseModel, Field
from langchain_community.tools.tavily_search import TavilySearchResults
from pprint import pprint
from dotenv import load_dotenv
from langgraph.graph import END, StateGraph, START
from pymilvus import MilvusClient, DataType, Collection
from langchain.agents import create_agent
from langchain_core.messages import HumanMessage
from langchain.tools import tool
from fastapi import FastAPI
from typer import prompt
load_dotenv()

# FastAPI app
app = FastAPI()

# Define the state for the multi-agent system
class MultiAgentState(TypedDict):
    question: str
    local_answer: str
    web_answer: str
    final_answer: str
    next_agent: str

#Milvus setup will be done in a different class so it can be reused

#Milvus setup is done on Vectordb_Creator.py
#The collection was created there as well
#So here we will just receive the collection name to use, making it so we can use different collections if needed

#Document loading from sops folder is done on Vectordb_PDFReader.py
#And then the data is loaded into Milvus on Vectordb_InfoLoader.py

#Technically the whole indexing process is done on those 3 files, creating, reading the pdfs and then loading into Milvus




# Define the LLM to be used by all agents
#For now we will use the same model for all agents, but this can be changed later if needed
#By setting up an endpoint to receive the model name
llm = ChatOllama(model="gpt-oss:20b", temperature=0)

# Initialize embeddings model (reuse for queries)
embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Define Tools for agents

@tool
def search_local_docs(query: str, collection_name: str) -> str:
    """Search for an answer to the query in the local documents."""
    
    # Generate embedding for the query
    query_vector = embeddings_model.embed_query(query)
    
    # Search in Milvus
    search_results = collection_name.search(
        collection_name=collection_name,
        data=[query_vector],
        limit=3,  # Top 3 most relevant documents
        output_fields=["text", "metadata"]
    )
    
    if not search_results or not search_results[0]:
        return "No relevant SOPs found in the local database."
    
    # Format results
    context = []
    for i, hit in enumerate(search_results[0], 1):
        context.append(f"Document {i} (score: {hit['distance']:.3f}):\n{hit['entity']['text']}\n")
    
    return "\n".join(context)

#This tool will be used by our second agent, the web search agent
#In this specific scenario, this agent shouldnt be used often, since most questions should be on the SOPs
@tool
def search_web(query: str) -> str:
    """Search the web for the answer to the question requested by the user."""
    
    web_search_tool = TavilySearchResults(k=3)
    results = web_search_tool.invoke(query)
    
    formatted_results = []
    for i, result in enumerate(results, 1):
        formatted_results.append(f"Result {i}:\n{result.get('content', '')}\nSource: {result.get('url', '')}\n")
    
    return "\n".join(formatted_results)

@tool
def synthesize_answers(local_answer: str, web_answer: str) -> str:
    """Synthesize answers from local documents and web search."""
    synthesis_prompt = ChatPromptTemplate.from_template(
        """You are an expert at combining information from multiple sources.
You will receive:
- Results from a local document search (authoritative, always prioritize)
- Results from web search (supplementary, use for context or missing info)
Your task:
1. **Prioritize local information** - This is official and should be your primary answer
2. **Enrich with web info** - Only add web details if they provide useful context or the local docs are incomplete
3. **Create a unified answer** - Combine both seamlessly, don't just list them separately
4. **Cite sources** - Mention which local document and which web sources you used"""
    )
    synthesis_input = synthesis_prompt.format(
        local_answer=local_answer,
        web_answer=web_answer
    )
    return llm(synthesis_input)


# Creating specialized sub-agents
#Here is the big difference. We will receive the collection name to use for the local agent
#As well as the prompt for the agent via FastAPI endpoint
#This means, we have no idea what the agent will be specialized in  

#Memory for rag configurations
rag_sessions: dict[str, dict] = {}

class AgentConfig(BaseModel):
    system_prompt: str
    collection_name: str
    session_id: Optional[str] = None

class QueryRequest(BaseModel):
    session_id: str
    question: str

# Router functions for the multi-agent system
def router_node(state: MultiAgentState) -> MultiAgentState:
    """Decide which agent to call first based on the question"""
    print("\n ROUTER: Analyzing question...")
    # Simple heuristic: if question seems like it needs local docs, start with local
    return {**state, "next_agent": "local"}

def route_decision(state: MultiAgentState) -> str:
    """Return which path to take from router"""
    return state.get("next_agent", "local")

def decide_next_step(state: MultiAgentState) -> str:
    """Decide if we need synthesis or can finalize"""
    # If we have both answers, synthesize them
    if state.get("local_answer") and state.get("web_answer"):
        return "synthesis"
    return "end"

def finalize_answer(state: MultiAgentState) -> MultiAgentState:
    """Finalize the answer when no synthesis is needed"""
    final = state.get("local_answer") or state.get("web_answer") or "No answer found."
    return {**state, "final_answer": final}

#Get the system prompt from the user via FastAPI endpoint

@app.post("/create_multi_agent")
def create_multi_agent(config: AgentConfig):
    """Creates a custom multi-agent RAG system with the given configuration"""
    
    #Generate or use the given session id
    session_id = config.session_id or str(uuid.uuid4())

    #Check the collection exists
    try:
        client = MilvusClient(
            uri="http://localhost:19530",
            token="root:Milvus",
            db_name="rag_db"
        )
        collections = client.list_collections()
        if config.collection_name not in collections:
            return {
                "error": f"Collection '{config.collection_name}' not found",
                "available_collections": collections
            }
    except Exception as e:
        return {"error": f"Failed to connect to Milvus: {str(e)}"}
    
    #Create custom local search for this specific collection
    def create_custom_local_search(collection: str):
        @tool
        def search_local_docs_custom(query: str) -> str:
            """Search for an answer to the query in the configured collection."""
            
            query_vector = embeddings_model.embed_query(query)
            
            client = MilvusClient(
                uri="http://localhost:19530",
                token="root:Milvus",
                db_name="rag_db"
            )
            search_results = client.search(
                collection_name=collection,
                data=[query_vector],
                limit=3,
                output_fields=["text", "metadata"]
            )
            
            if not search_results or not search_results[0]:
                return f"No relevant documents found in {collection}."
            
            context = []
            for i, hit in enumerate(search_results[0], 1):
                context.append(
                    f"Document {i} (score: {hit.get('distance', 0):.3f}):\n"
                    f"{hit['entity']['text']}\n"
                )
            
            return "\n".join(context)
        
        return search_local_docs_custom
    
    #Create specialized agents with the custom configuration
    local_search_tool = create_custom_local_search(config.collection_name)

    custom_local_agent = create_agent(
        llm,
        tools=[local_search_tool],
        system_prompt=config.system_prompt,
    )

    custom_web_agent = create_agent(
        llm,
        tools=[search_web],
        system_prompt="You are a web search specialist. Search the internet for relevant information.",
    )
    
    custom_synthesis_agent = create_agent(
        llm,
        tools=[synthesize_answers],
        system_prompt="You are an expert at synthesizing information from multiple sources.",
    )

    # Create the graph
    workflow = StateGraph(MultiAgentState)

    #Create custom nodes for the custom agents
    def custom_local_node(state: MultiAgentState) -> MultiAgentState:
        print(f"\n Calling LOCAL AGENT (searching {config.collection_name})...")
        result = custom_local_agent.invoke(
            {"messages": [HumanMessage(content=state["question"])]}
        )
        local_answer = result['messages'][-1].content
        return {**state, "local_answer": local_answer, "next_agent": "web"}
    
    def custom_web_node(state: MultiAgentState) -> MultiAgentState:
        print("\n Calling WEB AGENT...")
        result = custom_web_agent.invoke(
            {"messages": [HumanMessage(content=state["question"])]}
        )
        web_answer = result['messages'][-1].content
        return {**state, "web_answer": web_answer}
    
    def custom_synthesis_node(state: MultiAgentState) -> MultiAgentState:
        print("\n Calling SYNTHESIS AGENT...")
        synthesis_input = f"""Question: {state["question"]}

Local Answer from {config.collection_name}:
{state.get("local_answer", "Not available")}

Web Answer:
{state.get("web_answer", "Not available")}

{config.system_prompt}

Combine these into a final answer."""
        
        result = custom_synthesis_agent.invoke(
            {"messages": [HumanMessage(content=synthesis_input)]}
        )
        final_answer = result['messages'][-1].content
        return {**state, "final_answer": final_answer}
    
    # Graph structure
    workflow.add_node("router", router_node)
    workflow.add_node("local", custom_local_node)
    workflow.add_node("web", custom_web_node)
    workflow.add_node("synthesis", custom_synthesis_node)
    workflow.add_node("finalize", finalize_answer)
    
    workflow.add_edge(START, "router")
    workflow.add_conditional_edges("router", route_decision, {"local": "local", "web_only": "web"})
    workflow.add_edge("local", "web")
    workflow.add_conditional_edges("web", decide_next_step, {"synthesis": "synthesis", "end": "finalize"})
    workflow.add_edge("synthesis", END)
    workflow.add_edge("finalize", END)
    
    compiled_system = workflow.compile()
    
    # Save the complete configuration
    rag_sessions[session_id] = {
        "collection_name": config.collection_name,
        "system_prompt": config.system_prompt,
        "system": compiled_system,
        "created_at": str(uuid.uuid1().time)
    }
    
    return {
        "session_id": session_id,
        "message": "Multi-agent RAG system created successfully",
        "collection_name": config.collection_name,
        "system_prompt_preview": config.system_prompt[:100] + "..."
    }

@app.post("/query")
def query_rag(request: QueryRequest):
    """Query a configured RAG system using its session_id"""
    
    if request.session_id not in rag_sessions:
        return {"error": f"Session {request.session_id} not found. Create a system first."}
    
    session = rag_sessions[request.session_id]
    system = session["system"]
    
    print(f"\n Querying session {request.session_id} (collection: {session['collection_name']})")
    
    result = system.invoke({
        "question": request.question,
        "local_answer": "",
        "web_answer": "",
        "final_answer": "",
        "next_agent": ""
    })
    
    return {
        "session_id": request.session_id,
        "collection_name": session["collection_name"],
        "question": request.question,
        "answer": result["final_answer"]
    }

@app.get("/sessions")
def list_sessions():
    """List all active RAG sessions"""
    return {
        "total_sessions": len(rag_sessions),
        "sessions": [
            {
                "session_id": sid,
                "collection_name": info["collection_name"],
                "system_prompt_preview": info["system_prompt"][:80] + "...",
                "created_at": info["created_at"]
            }
            for sid, info in rag_sessions.items()
        ]
    }

@app.delete("/sessions/{session_id}")
def delete_session(session_id: str):
    """Delete a RAG session"""
    if session_id in rag_sessions:
        del rag_sessions[session_id]
        return {"message": f"Session {session_id} deleted successfully"}
    return {"error": f"Session {session_id} not found"}